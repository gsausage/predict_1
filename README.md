# predict_1
!pip install arch optuna
import yfinance as yf
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings
from arch import arch_model
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, classification_report
from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
import optuna

warnings.filterwarnings("ignore")

# ì„¤ì •
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

analysis_start_date = "2003-08-01"
end_date = "2023-08-01"
buffer_days = 150
window_size = 100
forecast_horizon = 1
window_len = 30

download_start_date = (pd.to_datetime(analysis_start_date) - pd.tseries.offsets.BDay(buffer_days)).strftime('%Y-%m-%d')
assets = {
    'KOSPI': '^KS11', 'S&P500': '^GSPC', 'SSE': '000001.SS', 'Gold': 'GC=F',
    'CrudeOil': 'CL=F', 'USD_KRW': 'KRW=X', 'VIX': '^VIX', 'US10Y': '^TNX'
}

# ë°ì´í„° ë‹¤ìš´ë¡œë“œ
raw_data = yf.download(list(assets.values()), start=download_start_date, end=end_date)['Close']
raw_data.columns = assets.keys()
data = raw_data.dropna()
returns = np.log(data).diff().dropna()

from scipy.stats import norm

def compute_var_es(series, alpha=0.01):  # 99% ê¸°ì¤€
    mu = series.mean()
    sigma = series.std()
    var = norm.ppf(alpha) * sigma
    es = mu - sigma * norm.pdf(norm.ppf(alpha)) / alpha
    return var, es

# 30ì¼ ì´ë™ ìœˆë„ìš° ê¸°ì¤€ìœ¼ë¡œ VaR & ES ê³„ì‚°
window = 30
var_list, es_list, idx_list = [], [], []
for i in range(window, len(returns)):
    window_data = returns['KOSPI'].iloc[i - window:i]
    var, es = compute_var_es(window_data)
    var_list.append(var)
    es_list.append(es)
    idx_list.append(returns.index[i])

var_es_df = pd.DataFrame({
    'VaR': var_list,
    'ES': es_list
}, index=idx_list)


# GJR-GARCH í‘œì¤€í™” ì”ì°¨ ê³„ì‚°
def get_standardized_resid(returns):
    std_resids = {}
    for col in returns.columns:
        am = arch_model(returns[col]*100, vol='Garch', p=1, o=1, q=1, dist='t')
        res = am.fit(disp='off')
        std_resids[col] = res.resid / res.conditional_volatility
    return pd.DataFrame(std_resids).dropna()

standardized_resid = get_standardized_resid(returns)

# DCC-GARCH ìƒê´€ê³„ìˆ˜ ì˜ˆì¸¡ í•¨ìˆ˜
def dcc_forecast_estimated(eps_window):
    T, N = eps_window.shape
    Qbar = np.cov(eps_window.T)
    def dcc_likelihood(params):
        a, b = params
        if a < 0 or b < 0 or a + b >= 1: return 1e6
        Qt = Qbar.copy()
        loglik = 0
        for t in range(1, T):
            et = eps_window[t-1].reshape(-1, 1)
            Qt = (1 - a - b) * Qbar + a * (et @ et.T) + b * Qt
            D = np.diag(1 / np.sqrt(np.diag(Qt)))
            Rt = D @ Qt @ D
            eps_t = eps_window[t].reshape(-1, 1)
            loglik += np.log(np.linalg.det(Rt)) + eps_t.T @ np.linalg.inv(Rt) @ eps_t
        return loglik.flatten()[0]
    from scipy.optimize import minimize
    res = minimize(dcc_likelihood, x0=[0.03, 0.94], bounds=[(0,1), (0,1)])
    a_hat, b_hat = res.x
    Qt = Qbar.copy()
    for t in range(T):
        et = eps_window[t].reshape(-1, 1)
        Qt = (1 - a_hat - b_hat) * Qbar + a_hat * (et @ et.T) + b_hat * Qt
    et = eps_window[-1].reshape(-1, 1)
    Q_forecast = (1 - a_hat - b_hat) * Qbar + a_hat * (et @ et.T) + b_hat * Qt
    D = np.diag(1 / np.sqrt(np.diag(Q_forecast)))
    return D @ Q_forecast @ D

# DCC ì˜ˆì¸¡ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
import os
import pickle

# ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
checkpoint_dir = "./checkpoints"
os.makedirs(checkpoint_dir, exist_ok=True)
dcc_path = os.path.join(checkpoint_dir, "dcc_forecast.pkl")

# 1. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸
if os.path.exists(dcc_path):
    with open(dcc_path, "rb") as f:
        dcc_corrs, forecast_dates = pickle.load(f)
    print("âœ… DCC forecast ë¡œë“œ ì™„ë£Œ (checkpoint ì‚¬ìš©ë¨)")
else:
      dcc_corrs, forecast_dates = {}, []
      extra_pairs = [('S&P500', 'VIX'), ('USD_KRW', 'US10Y')]
      for i in tqdm(range(window_size, len(standardized_resid) - forecast_horizon)):
          window = standardized_resid.iloc[i - window_size:i]
          R = dcc_forecast_estimated(window.values)
          forecast_date = standardized_resid.index[i + forecast_horizon]
          for col in returns.columns:
              if col == 'KOSPI': continue
              key = f'KOSPI-{col}'
              dcc_corrs.setdefault(key, []).append(R[returns.columns.get_loc('KOSPI'), returns.columns.get_loc(col)])
          for a1, a2 in extra_pairs:
              idx1, idx2 = returns.columns.get_loc(a1), returns.columns.get_loc(a2)
              key = f'{a1}-{a2}'
              dcc_corrs.setdefault(key, []).append(R[idx1, idx2])
          forecast_dates.append(forecast_date)
# 3. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
      with open(dcc_path, "wb") as f:
          pickle.dump((dcc_corrs, forecast_dates), f)
          print("âœ… DCC forecast ê³„ì‚° ì™„ë£Œ ë° checkpoint ì €ì¥ë¨")
# 2. forecast_df ì´ˆê¸°í™”
forecast_df = pd.DataFrame(dcc_corrs, index=forecast_dates)
forecast_df = forecast_df[forecast_df.index >= analysis_start_date]

# 3. VaR & ES ë³‘í•©
forecast_df = forecast_df.merge(var_es_df, left_index=True, right_index=True, how='left')
forecast_df = forecast_df.dropna()

#  KOSPI ìˆ˜ìµë¥  ê¸°ì¤€ ì¶©ê²©ì¼ ì •ì˜ (-2% ì´ìƒ í•˜ë½)
kospi_returns = returns['KOSPI']
shock_dates = kospi_returns[kospi_returns <= -0.02].index
print(f" ì¶©ê²©ì¼ ê°œìˆ˜ (KOSPI -2% ì´ìƒ í•˜ë½): {len(shock_dates)}ì¼")

# 5. ìƒê´€ê³„ìˆ˜ ì¤‘ìš” ë³€ìˆ˜ ì„ íƒ (RandomForest)
y_label = forecast_df.index.isin(shock_dates).astype(int)
df_clean = forecast_df.dropna()
y_label = y_label[-len(df_clean):]
X_scaled = StandardScaler().fit_transform(df_clean.values)
rf = RandomForestClassifier(n_estimators=200, random_state=SEED).fit(X_scaled, y_label)
importances = rf.feature_importances_# 6. ì£¼ìš” ìƒê´€ê³„ìˆ˜ + VaR + ES í¬í•¨í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
top_corr_cols = df_clean.columns[np.argsort(importances)][-5:]
final_cols = list(top_corr_cols) + ['VaR', 'ES']
forecast_df = forecast_df[final_cols]

# LSTM ì…ë ¥ ë°ì´í„° êµ¬ì„± ë° ì‹œê³„ì—´ ê¸°ë°˜ ìë™ ë¶„í• 
shock_extended = set()
for date in shock_dates:
    for offset in range(-2, 1):
        shock_extended.add(date + pd.Timedelta(days=offset))
shock_dates_extended = pd.DatetimeIndex(sorted(shock_extended))

forecast_idx = forecast_df.index
X_train, y_train, X_val, y_val, X_test, y_test = [], [], [], [], [], []

for idx in range(window_len, len(forecast_df)):
    current_date = forecast_idx[idx]
    window = forecast_df.iloc[idx - window_len: idx].values
    if window.shape[0] != window_len:
        continue
    label = 1 if current_date in shock_dates_extended else 0

    # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìë™ ë¶„í• 
    if current_date <= pd.Timestamp('204-12-31'):
        X_train.append(window)
        y_train.append(label)
    elif current_date <= pd.Timestamp('2016-06-30'):
        X_val.append(window)
        y_val.append(label)
    else:
        X_test.append(window)
        y_test.append(label)

# ë¦¬ìŠ¤íŠ¸ë¥¼ numpy arrayë¡œ ë³€í™˜
X_train, y_train = np.array(X_train), np.array(y_train)
X_val, y_val = np.array(X_val), np.array(y_val)
X_test, y_test = np.array(X_test), np.array(y_test)

# Jitterë¥¼ í™œìš©í•œ ì¦ê°•
import numpy as np
np.random.seed(SEED)

def jitter(x, sigma=0.01):
    noise = np.random.normal(loc=0.0, scale=sigma, size=x.shape)
    return x + noise

augmented_X, augmented_y = [], []
for xi, yi in zip(X_train, y_train):
    if yi == 1:
        for _ in range(10):  # 5ë°° ì¦ê°•
            augmented_X.append(jitter(xi))
            augmented_y.append(1)

# numpy arrayë¡œ ë³€í™˜ í›„ ë³‘í•©
X_train = np.concatenate([X_train, np.array(augmented_X)], axis=0)
y_train = np.concatenate([y_train, np.array(augmented_y)], axis=0)

import tensorflow.keras.backend as K
import tensorflow as tf

def focal_loss(gamma=1.5, alpha=0.9):
    def focal_loss_fixed(y_true, y_pred):
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)
        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)
        loss = -alpha_t * K.pow(1 - p_t, gamma) * K.log(p_t)
        return K.mean(loss)
    return focal_loss_fixed


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input

model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dropout(0.2),
    LSTM(32),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

# âœ… focal loss ì‚¬ìš© (í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘)
model.compile(
    optimizer='adam',
    loss=focal_loss(gamma=2.0, alpha=0.75),
    metrics=['accuracy']
)



class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(zip(np.unique(y_train), class_weights))
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
model.fit(
    X_train, y_train,
    epochs=200,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[early_stop],
    class_weight=class_weight_dict,
    verbose=1
)

# ì˜ˆì¸¡ ë° í‰ê°€
pred_dates, pred_probs = [], []
for i in range(window_len, len(forecast_df)):
    window = forecast_df.iloc[i - window_len:i].values
    if window.shape[0] != window_len:
        continue
    prob = model.predict(window.reshape(1, window_len, -1), verbose=0)[0][0]
    pred_probs.append(prob)
    pred_dates.append(forecast_df.index[i])
pred_result_df = pd.DataFrame({'date': pred_dates, 'prob': pred_probs}).set_index('date')
pred_result_df['true_label'] = pred_result_df.index.isin(shock_dates_extended).astype(int)

# 7. Optuna + F2 ìµœì í™”
import optuna
from sklearn.metrics import recall_score

# ğŸ” ê²€ì¦ì…‹ ë²”ìœ„ (validation set: 2023-01-01 ~ 2024-06-30)
val_mask = (pred_result_df.index >= pd.Timestamp("2014-01-01")) & (pred_result_df.index <= pd.Timestamp("2016-06-30"))
val_df = pred_result_df[val_mask].copy()

from sklearn.metrics import fbeta_score
def filter_consecutive_ones(series, min_days):
    result = pd.Series(0, index=series.index)
    count = 0

    for i in range(len(series)):
        if series.iloc[i] == 1:
            count += 1
        else:
            if count >= min_days:
                result.iloc[i - count:i] = 1
            count = 0
    # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ê°€ ì¡°ê±´ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸
    if count >= min_days:
        result.iloc[len(series) - count:] = 1

    return result

def objective(trial):
    threshold = trial.suggest_float("threshold", 0.01, 0.6)
    delta = trial.suggest_float("delta", 0, 0.1)
    corr_thresh = trial.suggest_float("corr_thresh", 0, 0.02)
    min_consecutive_days = trial.suggest_int("min_consecutive_days", 1, 5)  # ì¶”ê°€ë¨

    val_df['prev_mean'] = val_df['prob'].rolling(5, min_periods=1).mean()
    rule_filter = (
        (val_df['prob'] > threshold) &
        ((val_df['prob'] - val_df['prev_mean']) > delta)
    )

    dcc_vol = forecast_df.diff().abs().mean(axis=1)
    val_df['dcc_volatility'] = dcc_vol.reindex(val_df.index)
    dcc_filter = val_df['dcc_volatility'] > corr_thresh

    val_df['predicted_raw'] = (rule_filter & dcc_filter).astype(int)

    # âœ… ì—°ì†ëœ 1ì´ min_consecutive_days ì´ìƒì¸ ê²½ìš°ë§Œ ê²½ê³ ë¡œ ì¸ì •
    val_df['predicted_label'] = filter_consecutive_ones(val_df['predicted_raw'], min_consecutive_days)

    return fbeta_score(val_df['true_label'], val_df['predicted_label'], beta=1.5)


# âœ… Optuna ì‹¤í–‰
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=1000)
best_params = study.best_params
print("\nğŸ“Œ Optuna Best Params (from Validation Set):", best_params)

# ì „ì²´ ë°ì´í„°ì— best_params ì ìš©í•˜ì—¬ ë¼ë²¨ë§
pred_result_df['prev_mean'] = pred_result_df['prob'].rolling(5, min_periods=1).mean()
rule_filter = (
    (pred_result_df['prob'] > best_params['threshold']) &
    ((pred_result_df['prob'] - pred_result_df['prev_mean']) > best_params['delta'])
)
dcc_cols = [col for col in forecast_df.columns if '-' in col]
rolling_std = forecast_df[dcc_cols].rolling(window=5, min_periods=1).std()
pred_result_df['dcc_volatility'] = rolling_std.mean(axis=1).reindex(pred_result_df.index)
dcc_filter = pred_result_df['dcc_volatility'] > best_params['corr_thresh']
pred_result_df['predicted_label'] = (rule_filter & dcc_filter).astype(int)


# í…ŒìŠ¤íŠ¸ì…‹ ë§ˆìŠ¤í¬ ìƒì„±
test_mask = pred_result_df.index >= pd.Timestamp("2016-07-01")
test_df = pred_result_df[test_mask]

print("\nğŸ“Š í…ŒìŠ¤íŠ¸ì…‹ í‰ê°€ ê²°ê³¼:")
print(classification_report(test_df['true_label'], test_df['predicted_label']))
print("AUC:", roc_auc_score(test_df['true_label'], test_df['prob']))

# ë§ˆì§€ë§‰ ë‚ ì§œ ì •ë³´ ì¶œë ¥ (í…ŒìŠ¤íŠ¸ì…‹ ë‚´ì—ì„œ)
last_date = test_df.index.max()
row = test_df.loc[last_date]
print(f"\në§ˆì§€ë§‰ í…ŒìŠ¤íŠ¸ì…‹ ë‚ ì§œ: {last_date.date()}")
print(f"ì˜ˆì¸¡ í™•ë¥ : {row['prob']:.4f}")
print(f"ì´ì „ í‰ê· : {row['prev_mean']:.4f}")
print(f"DCC ë³€ë™ì„±: {row['dcc_volatility']:.4f}")
print(f"ì˜ˆì¸¡ ë¼ë²¨: {'ê²½ê³ ' if row['predicted_label']==1 else 'ë¹„ê²½ê³ '}")
print(f"ì‹¤ì œ ë¼ë²¨: {'ì¶©ê²©ì¼' if row['true_label']==1 else 'ë¹„ì¶©ê²©ì¼'}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
# Confusion Matrix (í…ŒìŠ¤íŠ¸ì…‹ ê¸°ì¤€)
cm = confusion_matrix(test_df['true_label'], test_df['predicted_label'])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["ë¹„ì¶©ê²©ì¼", "ì¶©ê²©ì¼"])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix (í…ŒìŠ¤íŠ¸ì…‹ ê¸°ì¤€)")
plt.grid(False)
plt.tight_layout()
plt.show()

# 1. KOSPI ìˆ˜ìµë¥  ì •ë ¬ (pred_result_dfì— ë§ì¶¤)
returns_kospi_bt = returns['KOSPI'].reindex(pred_result_df.index).fillna(0)

# 2. í¬ì§€ì…˜ ì„¤ì •: ì˜ˆì¸¡ ê²½ë³´ ì—†ì„ ë•Œë§Œ ì£¼ì‹ ë³´ìœ 
position = (~pred_result_df['predicted_label'].astype(bool)).astype(int)

# 3. ì „ëµ ìˆ˜ìµë¥  ê³„ì‚°
strategy_returns = returns_kospi_bt * position

# 4. ëˆ„ì  ìˆ˜ìµë¥  ê³„ì‚°
cum_strategy = (strategy_returns + 1).cumprod()
cum_bh = (returns_kospi_bt + 1).cumprod()

# 5. ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì •ë¦¬
bt_result_df = pd.DataFrame({
    "Buy & Hold": cum_bh,
    "Model Strategy": cum_strategy
})
import matplotlib.pyplot as plt

# í…ŒìŠ¤íŠ¸ì…‹ë§Œ í•„í„°ë§
test_df = pred_result_df[pred_result_df.index >= pd.Timestamp("2016-07-01")].copy()

plt.figure(figsize=(15, 5))

# ì˜ˆì¸¡ í™•ë¥  ë¼ì¸
plt.plot(test_df.index, test_df['prob'], label='Predicted Probability', color='black', linewidth=1)

# Threshold ë¼ì¸ (Optunaì—ì„œ ì°¾ì€ best threshold)
plt.axhline(best_params['threshold'], color='red', linestyle='--', label=f"Threshold = {best_params['threshold']:.2f}")

# ì¶©ê²©ì¼ í‘œì‹œ
shock_dates = test_df[test_df['true_label'] == 1].index
plt.scatter(shock_dates, test_df.loc[shock_dates, 'prob'], color='blue', label='Actual Shock', marker='x', s=80)

# ì˜ˆì¸¡ëœ ê²½ê³ ì¼ í‘œì‹œ
predicted_shocks = test_df[test_df['predicted_label'] == 1].index
plt.scatter(predicted_shocks, test_df.loc[predicted_shocks, 'prob'], color='orange', label='Predicted Warning', marker='o', facecolors='none', edgecolors='orange', s=80)

# ì‹œê°ì  ìš”ì†Œ
plt.title("í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì¸¡ í™•ë¥  ì‹œê³„ì—´")
plt.xlabel("Date")
plt.ylabel("Predicted Probability")
plt.legend(loc='upper right')
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 4))
pred_result_df['prob'].hist(bins=50, color='skyblue', edgecolor='black')
plt.title("LSTM ì˜ˆì¸¡ í™•ë¥  ë¶„í¬")
plt.xlabel("Predicted Probability")
plt.ylabel("Frequency")
plt.grid(True)
plt.tight_layout()
plt.show()

# 6. ì‹œê°í™”
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
plt.plot(bt_result_df.index, bt_result_df["Buy & Hold"], label="Buy & Hold", linestyle='--')
plt.plot(bt_result_df.index, bt_result_df["Model Strategy"], label="Model Strategy", linewidth=2)
plt.title(" ëˆ„ì  ìˆ˜ìµë¥  ë¹„êµ (ë°±í…ŒìŠ¤íŠ¸)")
plt.xlabel("Date")
plt.ylabel("Cumulative Return")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

def compare_hedged_vs_original_portfolio_script(pred_result_df, forecast_df, returns, window=60):
    import matplotlib.pyplot as plt

    def construct_partial_cov_hedge_portfolio(forecast_df, returns, target_asset='KOSPI', date=None, window=60):
        if date is None:
            date = forecast_df.index[-1]
        else:
            date = pd.to_datetime(date)
            if date not in forecast_df.index:
                date = forecast_df.index[forecast_df.index.get_indexer([date], method='nearest')[0]]

        hedge_assets = [col.split('-')[1] for col in forecast_df.columns if col.startswith(f'{target_asset}-')]
        all_assets = [target_asset] + hedge_assets

        aligned_returns = returns[all_assets].dropna()
        recent_returns = aligned_returns.loc[:date].iloc[-window:]
        std_devs = recent_returns.std()

        corr_matrix = np.eye(len(all_assets))
        asset_idx = {a: i for i, a in enumerate(all_assets)}
        for col in forecast_df.columns:
            a1, a2 = col.split('-')
            if a1 == target_asset and a2 in hedge_assets:
                i, j = asset_idx[a1], asset_idx[a2]
                corr = forecast_df.loc[date, col]
                corr_matrix[i, j] = corr
                corr_matrix[j, i] = corr

        cov_matrix = np.outer(std_devs.values, std_devs.values) * corr_matrix

        sigma_kk = cov_matrix[0, 0]
        sigma_kw = cov_matrix[0, 1:]
        sigma_ww = cov_matrix[1:, 1:]

        opt_w = -np.linalg.pinv(sigma_ww) @ sigma_kw.T
        var_hedged = sigma_kk + 2 * opt_w.T @ sigma_kw + opt_w.T @ sigma_ww @ opt_w

        return {
            'date': date,
            'target_variance': sigma_kk,
            'hedged_variance': float(var_hedged),
            'reduction_ratio': 1 - float(var_hedged) / sigma_kk,
            'weights': dict(zip(hedge_assets, opt_w))
        }

    shock_dates = pred_result_df[pred_result_df['predicted_label'] == 1].index
    hedge_returns = []
    kospi_returns = []

    for shock_day in shock_dates:
        if shock_day not in forecast_df.index or shock_day not in returns.index:
            continue

        hedge_result = construct_partial_cov_hedge_portfolio(
            forecast_df=forecast_df,
            returns=returns,
            target_asset='KOSPI',
            date=shock_day,
            window=window
        )

        weights = hedge_result['weights']
        aligned_assets = ['KOSPI'] + list(weights.keys())

        try:
            one_day_returns = returns.loc[shock_day, aligned_assets]
        except KeyError:
            continue

        hedge_r = one_day_returns['KOSPI'] + sum(
            one_day_returns[asset] * w for asset, w in weights.items()
        )

        hedge_returns.append(hedge_r)
        kospi_returns.append(one_day_returns['KOSPI'])

    hedge_returns = pd.Series(hedge_returns, index=shock_dates[:len(hedge_returns)])
    kospi_returns = pd.Series(kospi_returns, index=shock_dates[:len(kospi_returns)])

    # ëˆ„ì  ìˆ˜ìµë¥ 
    cum_kospi = (1 + kospi_returns).cumprod()
    cum_hedged = (1 + hedge_returns).cumprod()

    # ì‹œê°í™”: ëˆ„ì  ìˆ˜ìµë¥ 
    plt.figure(figsize=(12, 6))
    plt.plot(cum_kospi, label="KOSPI ë‹¨ë… (ì¶©ê²©ì¼)", linewidth=2)
    plt.plot(cum_hedged, label="í—·ì§€ í¬íŠ¸í´ë¦¬ì˜¤ (ì¶©ê²©ì¼)", linewidth=2, linestyle='--')
    plt.title("ì¶©ê²©ì¼ ëˆ„ì  ìˆ˜ìµë¥  ë¹„êµ")
    plt.ylabel("ëˆ„ì  ìˆ˜ìµë¥ ")
    plt.xlabel("ë‚ ì§œ")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    # ì‹œê°í™”: ë¶„ì‚° ë¹„êµ
    plt.figure(figsize=(10, 4))
    plt.bar(['KOSPI'], [kospi_returns.var()], label='KOSPI')  
    plt.bar(['í—·ì§€ í¬íŠ¸í´ë¦¬ì˜¤'], [hedge_returns.var()], label='í—·ì§€ í¬íŠ¸í´ë¦¬ì˜¤')
    plt.title("ì¶©ê²©ì¼ ìˆ˜ìµë¥  ë¶„ì‚° ë¹„êµ")
    plt.ylabel("ë¶„ì‚°")
    plt.grid(axis='y')
    plt.tight_layout()
    plt.show()

    return kospi_returns.describe(), hedge_returns.describe()






